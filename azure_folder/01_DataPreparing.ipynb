{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# DataPreparing\n",
        "\n",
        "THe first step of our notebooks will be to run a DataPreparing script.  \n",
        "This contains all the necessary code to transform our original sequences into data that is ready for AI training.  \n",
        "\n",
        "To benefit from the perks of our Azure cloud service, we will be creating a new dataset to store our processed sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Setup\n",
        "\n",
        "Our virtual machine might not have all packages installed yet. So let's go and install some packages.  \n",
        "We can use cell-magic for this, which will allow us to stay inside this notebook and just executing the cells.  \n",
        "\n",
        "Later on, these cells might nog be necessary anymore, which is why we include it at the top. During other builds, you can just ignore these.\n",
        "\n",
        "As a best practice, let's make sure to only work on the version we know is safe. This is a great way to organising our AI projects. By keeping the versions linked like this, no unexpected new version would break our code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1644072535860
        }
      },
      "outputs": [],
      "source": [
        "# This cell can be used to fill in some values that you will be referring to in the coming cells\n",
        "train_test_split_factor = 0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1644072536895
        }
      },
      "outputs": [],
      "source": [
        "# Importing the default packages for data processing and visualisation\n",
        "import numpy as np # Used to process our sequences in a data-format\n",
        "from shutil import rmtree\n",
        "\n",
        "import os,math\n",
        "from glob import glob\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # Warnings that can be ignored will be ignored\n",
        "\n",
        "import random\n",
        "SEED = 42 # Everytime you want to randomize items, use this `random.seed(SEED)` option. This way, you are always having the same randomization as I have.\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1644072537973
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import AzureML packages\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Connecting to the Azure ML Workspace\n",
        "\n",
        "Azure Machine Learning needs to connect through the Azure SDK with the Workspace object. This contains all the information inside of this 'Laboratory'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The information below should reflect your situation regarding Azure. You should have a ResourceGroup called '04_AzureML' and a workspace name called 'segersnathan' if you followed my instructions on HackMD.\n",
        "The subscription ID, however, is something that has been created by Azure itself.\n",
        "\n",
        "Luckily, this ML studio gives us a quick way to find this information.\n",
        "Click on the \\/-arrow in the upper-right corner over there â†—ï¸, next to your profile picture.\n",
        "\n",
        "Most of your information is in there as well, but you still can't find your subscription_**id** there ...\n",
        "\n",
        "Press the 'Download config' option, and you'll be left with this information:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"subscription_id\": \"7c50f9c3-289b-4ae0-a075-08784b3b9042\",\n",
        "    \"resource_group\": \"NathanReserve\",\n",
        "    \"workspace_name\": \"segersnathan\"\n",
        "}\n",
        "```\n",
        "\n",
        "Which gives you exactly the information you need ðŸ¥°\n",
        "\n",
        "There's also an option to use this configuration itself. Search for the documentation on how to do it: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1644072540348
        }
      },
      "outputs": [],
      "source": [
        "## Either get environment variables, or a fallback name, which is the second parameter.\n",
        "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
        "workspace_name = os.environ.get('WORKSPACE', 'hermans-cedric-ml')\n",
        "subscription_id = os.environ.get('SUBSCRIPTION_ID', 'REDACTED')\n",
        "resource_group = os.environ.get('RESOURCE_GROUP', '04-AzureML')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1644072542576
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.get(name=workspace_name,\n",
        "               subscription_id=subscription_id,\n",
        "               resource_group=resource_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 -- Data preparing\n",
        "\n",
        "The initial dataset contains two types of sequences: polyketide synthases and non-ribosomal peptide synthase.\n",
        "\n",
        "## Step 2.1 -- Checking our data\n",
        "\n",
        "Let us first explore how the data looks. We'll create 2 subdirectories under a data directory, one for each gene.\n",
        "If you want to update this to more genes later, simply adapt the `GENES` list. Because PKS contains about 16000 sequences, and the \n",
        "webpage kept crashing when uploading the data,\n",
        "we have added some code to process the two type of sequences in different ways (same results, but PKS was catted to one file, NRPS are\n",
        "seperate files) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1644072543664
        }
      },
      "outputs": [],
      "source": [
        "GENES = ['NRPS', 'PKS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We will need to create temporary directories to store the sequences while we process them.\n",
        "This script will create a `data` folder, and then make subdirectories for each animal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1644072552751
        }
      },
      "outputs": [],
      "source": [
        "data_folder = os.path.join(os.getcwd(), 'data')\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "for gene_name in GENES:\n",
        "    os.makedirs(os.path.join(data_folder, 'genes', gene_name), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1644072554013
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ 'NRPS': DatasetRegistration(id='cd0e7d4c-fd07-4538-8df1-5344ac913e2b', name='NRPS', version=1, description='', tags={}),\n",
            "  'PKS': DatasetRegistration(id='019c4c3d-8a82-4260-98ce-2e3047dc8170', name='PKS', version=1, description='', tags={}),\n",
            "  'animals-testing-set': DatasetRegistration(id='b8ec1966-c9da-4ca9-b07d-e41ee35f5e07', name='animals-testing-set', version=1, description='The Animal Images to test, resized tot 64, 64', tags={'animals': 'cats,dogs,pandas', 'AI-Model': 'CNN', 'Split size': '0.2', 'type': 'testing'}),\n",
            "  'animals-training-set': DatasetRegistration(id='02ab0351-03ab-4e0e-b42f-620fa183bd4d', name='animals-training-set', version=1, description='The Animal Images to train, resized tot 64, 64', tags={'animals': 'cats,dogs,pandas', 'AI-Model': 'CNN', 'Split size': '0.8', 'type': 'training'}),\n",
            "  'cats': DatasetRegistration(id='db847d56-0389-43e8-93aa-6cd3df7507c7', name='cats', version=1, description='', tags={}),\n",
            "  'dogs': DatasetRegistration(id='1f40c908-e568-4434-a620-5b77cc638116', name='dogs', version=1, description='', tags={}),\n",
            "  'pandas': DatasetRegistration(id='c50be40f-bbb1-406a-8c39-1fc41d67f8dc', name='pandas', version=1, description='', tags={}),\n",
            "  'processed_NRPS': DatasetRegistration(id='9f2904c6-5432-43cc-9f87-173e8c9a387a', name='processed_NRPS', version=1, description='NRPS sequences processed to amino acid counts', tags={'genes': 'NRPS', 'AI-Model': 'NN'}),\n",
            "  'processed_PKS': DatasetRegistration(id='81f4fa96-08ae-478b-a845-a6bf7090a743', name='processed_PKS', version=1, description='PKS sequences processed to amino acid counts', tags={'genes': 'PKS', 'AI-Model': 'NN'}),\n",
            "  'processed_genes': DatasetRegistration(id='90c3f12c-cfe1-429d-b40a-59b50b2e18c9', name='processed_genes', version=1, description='Gene sequences processed to amino acid counts', tags={'genes': \"['NRPS', 'PKS']\", 'AI-Model': 'NN'}),\n",
            "  'resized_cats': DatasetRegistration(id='765d46f3-ce50-47aa-bae9-4875c1892305', name='resized_cats', version=1, description='cats images resized tot 64, 64', tags={'animals': 'cats', 'AI-Model': 'CNN'}),\n",
            "  'resized_dogs': DatasetRegistration(id='eb4cd0d9-f2b8-4537-b86a-0868ae1dd7be', name='resized_dogs', version=1, description='dogs images resized tot 64, 64', tags={'animals': 'dogs', 'AI-Model': 'CNN'}),\n",
            "  'resized_pandas': DatasetRegistration(id='4a83c752-3e9c-4ec8-b603-addacdb916de', name='resized_pandas', version=1, description='pandas images resized tot 64, 64', tags={'animals': 'pandas', 'AI-Model': 'CNN'})}\n"
          ]
        }
      ],
      "source": [
        "# Get all the datasets that were registered in the UI\n",
        "# We can then easily select the ones we need\n",
        "datasets = Dataset.get_all(workspace=ws) # Make sure to give our workspace with it\n",
        "print(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "A check to see if we have our datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1644072556851
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All datasets present!\n"
          ]
        }
      ],
      "source": [
        "# Write your answer here\n",
        "nomissing=True\n",
        "for gene in GENES:\n",
        "    if gene in datasets.keys():\n",
        "        continue\n",
        "    nomissing=False\n",
        "    print(\"Missing dataset %s\"%gene)\n",
        "if nomissing:\n",
        "    print(\"All datasets present!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2.2 Processing and uploading the processed sequences\n",
        "\n",
        "We need to process our sequences so we can use them in our model, which is just a normal Neural Network with 21 inputs (one for each amino acid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1644072558217
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Let's create a mounth point. Think of it like your D:/ drive on your PC\n",
        "mount_path = os.path.join(os.getcwd(), 'mount')\n",
        "os.makedirs(mount_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1644062464762
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1644073568339
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def mountProcessSequences(gene_name):\n",
        "    # Define a path to store the gene sequences onto. We'll choose for `data/processed/genes` this time. Again, create subdirectories for all the genes\n",
        "    processed_path = os.path.join(os.getcwd(), 'data', 'processed', 'genes')\n",
        "    os.makedirs(processed_path, exist_ok=True)\n",
        "    # The mount context is to load in the dataset to our directory.\n",
        "    # Make sure to stop it when it's finished!\n",
        "\n",
        "    # Get the dataset name for this gene, then mount to the directory\n",
        "    mounted_context = datasets[gene_name].mount(mount_path)\n",
        "    print('Starting the Mount context, to get all the original sequences.')\n",
        "    mounted_context.start()\n",
        "\n",
        "    # Get all the sequence paths with the `glob()` method.\n",
        "    print(f'Processing all sequences for {gene_name} ...')\n",
        "    sequencePaths = glob(f\"{mount_path}/**.fasta\")\n",
        "    print(len(sequencePaths))\n",
        "    aminoacids=[\"A\",\"R\",\"N\",\"D\",\"C\",\"E\",\"Q\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"Y\",\"V\"]\n",
        "    # reading and preprocessing of the sequences\n",
        "    X = []\n",
        "    y = []\n",
        "    # read  sequences\n",
        "    if gene_name == 'NRPS':\n",
        "        for seq in sequencePaths:\n",
        "            with open(seq) as ifile:\n",
        "                text=ifile.readlines()\n",
        "                sequence=''.join([l.strip() for l in text[1:]])\n",
        "                X.append([sequence.count(aa) for aa in aminoacids])\n",
        "                y.append([1,0])\n",
        "    elif gene_name == 'PKS':\n",
        "        for seq in sequencePaths:\n",
        "            print(seq)\n",
        "            with open(seq) as ifile:\n",
        "                text = ifile.read().split(\">\")[1:]\n",
        "                for s in text:\n",
        "                    sequence=''.join([l.strip() for l in s[1:]])\n",
        "                    X.append([sequence.count(aa) for aa in aminoacids])\n",
        "                    y.append([0,1])\n",
        "    print(len(X))\n",
        "    np.savetxt(os.path.join(processed_path, f\"{gene_name}_X.np\"), np.array(X))\n",
        "    np.savetxt(os.path.join(processed_path, f\"{gene_name}_y.np\"), np.array(y))\n",
        "        \n",
        "    \n",
        "    print(f'... Done')\n",
        "    # Stop the context now.\n",
        "    mounted_context.stop()\n",
        "    print(f\"... Context stopped and freed.\")\n",
        "\n",
        "def uploadGeneSequences():\n",
        "    processed_path = os.path.join(os.getcwd(), 'data', 'processed', 'genes')\n",
        "    # Upload the directory as a new dataset\n",
        "    print(f'Uploading directory now ...')\n",
        "    resized_dataset = Dataset.File.upload_directory(\n",
        "                        # Enter the sourece directory on our machine where the resized pictures are\n",
        "                        src_dir = processed_path,\n",
        "                        # Create a DataPath reference where to store our sequences to. We'll use the default datastore for our workspace.\n",
        "                        target = DataPath(datastore=ws.get_default_datastore(), path_on_datastore=f'processed_genes'),\n",
        "                        overwrite=True)\n",
        "    # Make sure to register the dataset whenever everything is uploaded.\n",
        "    resized_dataset.register(ws,\n",
        "                            name=f'processed_genes',\n",
        "                            description=f'Gene sequences processed to amino acid counts',\n",
        "                            tags={'genes': str(GENES), 'AI-Model': 'NN'}, # Optional tags, can always be interesting to keep track of these!\n",
        "                            create_new_version=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1640004384857
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading directory now ...\n",
            "Validating arguments.\n",
            "Arguments validated.\n",
            "Uploading file to processed_genes\n",
            "Uploading an estimated of 6 files\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/.amlignore\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/.amlignore, 1 files out of an estimated total of 6\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/.amlignore.amltmp\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/.amlignore.amltmp, 2 files out of an estimated total of 6\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/NRPS_X.np\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/NRPS_X.np, 3 files out of an estimated total of 6\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/NRPS_y.np\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/NRPS_y.np, 4 files out of an estimated total of 6\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/PKS_y.np\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/PKS_y.np, 5 files out of an estimated total of 6\n",
            "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/PKS_X.np\n",
            "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-ch/code/Users/cedric.hermans2/AzureML-assignment/data/processed/genes/PKS_X.np, 6 files out of an estimated total of 6\n",
            "Uploaded 6 files\n",
            "Creating new dataset\n",
            "CPU times: user 228 ms, sys: 28.6 ms, total: 257 ms\n",
            "Wall time: 6.13 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Process all the gene sequences now.\n",
        "# We'll use Cell magic once more, to time how long this takes!\n",
        "mountProcessSequences('NRPS')\n",
        "mountProcessSequences('PKS')\n",
        "uploadGeneSequences()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Our processed sequence datasets are now registered onto the datasets of Azure."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "5826022c13cf670f25f82c54601002af42321e6e77593e3891ed52ffe2de205c"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
